{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uILTSEsU41l"
      },
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "from numba import cuda\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2pvEGMDWAMJ",
        "outputId": "c1483d6c-cc15-4f85-c2fc-23e4c035ffd7"
      },
      "source": [
        "cuda.detect()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CUDA devices\n",
            "id 0            b'Tesla K80'                              [SUPPORTED]\n",
            "                      compute capability: 3.7\n",
            "                           pci device id: 4\n",
            "                              pci bus id: 0\n",
            "Summary:\n",
            "\t1/1 devices are supported\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ6WsfpnWExX",
        "outputId": "01ebf3b8-3430-44a2-dd1e-4b294b52c85c"
      },
      "source": [
        "#create a CPU numpy array (we suppose that we have these data only on CPU and we can not generate them directly on GPU)\n",
        "h_A = np.random.randint(0, 255, size=(1000, 1000))\n",
        "h_B = np.random.randint(0, 255, size=(1000, 1000))\n",
        "\n",
        "#we generate the result matrix directly on GPU\n",
        "d_C = cp.zeros((1000, 1000) , dtype=np.float64)\n",
        "\n",
        "#size of the cpu_array\n",
        "size_array_in_memory = (h_A.nbytes / 1e6) + (h_B.nbytes / 1e6)\n",
        "print(f\"the size in memory is:{size_array_in_memory} Mega byte\\n\")\n",
        "\n",
        "#Sending the array to device\n",
        "d_A = cuda.to_device(h_A)\n",
        "d_B = cuda.to_device(h_B)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the size in memory is:16.0 Mega byte\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZYY61j-eFT2"
      },
      "source": [
        "@cuda.jit\n",
        "def matmul(A, B, C):\n",
        "    \"\"\"\n",
        "      Perform square matrix multiplication of C = A * B\n",
        "      this kernel is executed on each element of our matrix\n",
        "    \"\"\"\n",
        "    i, j = cuda.grid(2)  \n",
        "    if i < C.shape[0] and j < C.shape[1]:   # grid can extend beyond C\n",
        "        tmp = 0.      \n",
        "        for k in range(A.shape[1]):\n",
        "            tmp += A[i, k] * B[k, j]        # multiply elements in row i of A and column j of B and add to temp\n",
        "        C[i, j] = tmp"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ourny9xNgEDd",
        "outputId": "b7a21836-cf35-49e0-9d70-0e45d50090e9"
      },
      "source": [
        "#Compute thread and grid to execute\n",
        "threadsperblock = (16, 16)  # each block will contain 16x16 threads, typically 128 - 512 threads/block\n",
        "blockspergrid_x = int(np.ceil(d_C.shape[0] / threadsperblock[0]))\n",
        "blockspergrid_y = int(np.ceil(d_C.shape[1] / threadsperblock[1]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)  # we calculate the gridsize (number of blocks) from array\n",
        "print(blockspergrid)\n",
        "print(f\"The kernel will be executed up to element {threadsperblock[0]*blockspergrid_x}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(63, 63)\n",
            "The kernel will be executed up to element 1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIb-nq3DgwcO",
        "outputId": "67836fe5-d439-4281-fe64-f5cb109c27a3"
      },
      "source": [
        "# execution of the kernel\n",
        "matmul[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
        "\n",
        "#print the data A with cupy\n",
        "print(cp.asarray(d_C))\n",
        "print(\"-------\\n\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15407195. 16137516. 15955002. ... 16327436. 15955913. 16199674.]\n",
            " [15855426. 16203021. 16429476. ... 16859972. 16359792. 17032604.]\n",
            " [16281369. 15744481. 16302085. ... 16641918. 16251003. 16837721.]\n",
            " ...\n",
            " [16033470. 16512866. 16728134. ... 16900391. 16746713. 16910387.]\n",
            " [15894458. 15649875. 16158911. ... 16114632. 15648543. 16198288.]\n",
            " [15957473. 15915692. 16382346. ... 16549592. 16131364. 17014892.]]\n",
            "-------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3NmJMHahiFK"
      },
      "source": [
        "# faster multiplication can be obtained by making use of shared memory between threads in the same block\n",
        "# this requires more thinking about non-obvious implementation!\n",
        "\n",
        "from numba import float32, int32, float64\n",
        "\n",
        "# Controls threads per block and shared memory usage.\n",
        "# The computation will be done on blocks of TPBxTPB elements.\n",
        "TPB = 16\n",
        "\n",
        "@cuda.jit\n",
        "def fast_matmul(A, B, C):\n",
        "    # Define an array in the shared memory\n",
        "    # The size and type of the arrays must be known at compile time\n",
        "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
        "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
        "\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    bpg = cuda.gridDim.x    # blocks per grid\n",
        "\n",
        "    if x >= C.shape[0] and y >= C.shape[1]:\n",
        "        # Quit if (x, y) is outside of valid C boundary\n",
        "        return\n",
        "\n",
        "    # Each thread computes one element in the result matrix.\n",
        "    # The dot product is chunked into dot products of TPB-long vectors.\n",
        "    tmp = 0.\n",
        "    for i in range(bpg):\n",
        "        # Preload data into shared memory\n",
        "        sA[tx, ty] = A[x, ty + i * TPB]\n",
        "        sB[tx, ty] = B[tx + i * TPB, y]\n",
        "\n",
        "        # Wait until all threads finish preloading\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Computes partial product on the shared memory\n",
        "        for j in range(TPB):\n",
        "            tmp += sA[tx, j] * sB[j, ty]\n",
        "\n",
        "        # Wait until all threads finish computing\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    C[x, y] = tmp"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQN2Rnq6kDiO",
        "outputId": "e3c2d63d-8775-4ed2-ddb4-adc0251d285e"
      },
      "source": [
        "# execution of the kernel\n",
        "fast_matmul[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
        "\n",
        "#print the data A with cupy\n",
        "print(cp.asarray(d_C))\n",
        "print(\"-------\\n\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15407195. 16137516. 15955002. ... 16327436. 15955913. 16199674.]\n",
            " [15682464. 15757583. 16270867. ... 16859972. 16359792. 17032604.]\n",
            " [15653544. 16160277. 16286697. ... 16641918. 16251003. 16837721.]\n",
            " ...\n",
            " [ 7444758.  7823575.  8036345. ... 16900391. 16746713. 16910387.]\n",
            " [ 8054330.  8626057.  8710958. ... 16114632. 15648543. 16198288.]\n",
            " [ 7744923.  8119763.  8262746. ... 16549592. 16131364. 17014892.]]\n",
            "-------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q79LVOhkSvM",
        "outputId": "874a3323-6457-4c65-bf43-b42033e52e81"
      },
      "source": [
        "#get back the matrix to host\n",
        "h_C = d_C.get()\n",
        "print(h_C)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15407195. 16137516. 15955002. ... 16327436. 15955913. 16199674.]\n",
            " [15682464. 15757583. 16270867. ... 16859972. 16359792. 17032604.]\n",
            " [15653544. 16160277. 16286697. ... 16641918. 16251003. 16837721.]\n",
            " ...\n",
            " [ 7444758.  7823575.  8036345. ... 16900391. 16746713. 16910387.]\n",
            " [ 8054330.  8626057.  8710958. ... 16114632. 15648543. 16198288.]\n",
            " [ 7744923.  8119763.  8262746. ... 16549592. 16131364. 17014892.]]\n"
          ]
        }
      ]
    }
  ]
}